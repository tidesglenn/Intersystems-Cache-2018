DrillTimeFive
; Traverses global structures and maps data to CSV rows based on class metadata.
; Compatible with Caché 2018. Uses %Dictionary.ClassDefinition for property metadata.
;
DrillTimeThree(pGlobal, pClass, pKeyList, pLogLevel, pWorker, pTotalWorkers)
    s $ZERROR=""
    s $ZTRAP=""
    set $ZTRAP="TRAP^DrillTimeFive"

    new keyLength, keyFields, fieldMap, classDef, sc, gLogLevel
    set gLogLevel=$get(pLogLevel,1)
    new seq, propName, propObj, skip, type, coll, outputProps, i, val, recCount, fieldNodeMap, fieldPieceMap, fieldTypeMap
    set recCount=0
    ; pKeyList, pGlobal and pClass can be passed in by the caller.  The lines
    ; below are examples only and are commented out so the routine works with
    ; any number of key pieces.
    ;s pKeyList="HMO,ID,RIN"
    ;s pGlobal="^[""MHRREG""]HMOREF"
    ;s pClass="Referrals.Referral"
    ; Determine key length from string or count
    set keyLength=1
    if pKeyList'="" {
    . if pKeyList?1N.N {
    . . set keyLength=+pKeyList
    . } else {
    . . set keyLength=$length(pKeyList, ",")
    . . for i=1:1:keyLength set keyFields(i)=$piece(pKeyList,",",i)
    . }
    }
    do Log("Worker "_$get(pWorker,1)_"/"_$get(pTotalWorkers,1)_": Starting export for "_pClass_" from "_pGlobal_": key length="_keyLength,1)

    ; Use ClassDefinition instead of CompiledClass for better compatibility
    set classDef=##class(%Dictionary.ClassDefinition).%OpenId(pClass,.sc)
    if '$isobject(classDef) {
    . write "ERROR: Class not found or could not open: ",pClass,! quit
    }

    ; Loop through all Properties safely using $ORDER
    set i=""
    s totalProps=classDef.Properties.Count()
    i (('$l($g(totalProps)))||(totalProps=0)) w !,"No Properties" q
    for i=1:1:totalProps {
     quit:i=""
     set propObj=classDef.Properties.GetAt(i)
     if '$isobject(propObj) continue
     set skip=0
     if $property(propObj,"Calculated") set skip=1
     if $property(propObj,"Relationship") set skip=1
     if $property(propObj,"MultiDimensional") set skip=1
     set coll=$property(propObj,"Collection") if coll'="" set skip=1
     set type=$property(propObj,"Type") if type["%Stream" set skip=1
     ;if $property(propObj,"Storable")=0 set skip=1
     if skip=1 continue
    set propName=$property(propObj,"Name")
    set seq=$property(propObj,"SequenceNumber") if seq="" set seq=9999
    set outputProps(seq,propName)=""
    s storageProps=classDef.Storages.GetAt(i)
    if ($l($g(storageProps))){
    new tmpNode,tmpPiece,loc
    set tmpNode=$$GetProp(storageProps,"Node")
    set tmpPiece=$$GetProp(storageProps,"Piece")
    if tmpNode="" {
        set loc=$$GetProp(storageProps,"DataLocation")
        if loc'="" {
            set tmpNode=$piece(loc,":",1)
            set tmpPiece=$piece(loc,":",2)
        }
    }
    set fieldNodeMap(propName)=tmpNode
    set fieldPieceMap(propName)=tmpPiece
    }
     set fieldTypeMap(propName)=type
    }

    ; Add CSV header line using proper $ORDER traversal
    new headerLine,seq,propName set headerLine="",seq=""
    for  set seq=$order(outputProps(seq)) quit:seq=""  {
        set propName=""
        for  set propName=$order(outputProps(seq,propName)) quit:propName=""  {
            set headerLine=headerLine_$select(headerLine'="":",","")_""""_propName_""""
        }
    }
    set ^EXPORT($J,0)=headerLine

    ; Begin recursive traversal from global root
    do Traverse(pGlobal, "", keyLength, pClass, .outputProps, .recCount, .fieldNodeMap, .fieldPieceMap, .fieldTypeMap)
    do WriteCSV(pClass,$get(pWorker,1))
    do Log("Export complete: "_recCount_" rows written to CSV.",1)
    quit

TRAP ; Basic error handler
    write "ERROR: ",$zerror,! quit

; Recursive global walker
Traverse(gref, subs, keyLength, className, outputProps, recCount, fieldNodeMap,fieldPieceMap, fieldTypeMap)
    new sub,prefix set sub=""
    set prefix=gref_"("
    if subs'="" set prefix=gref_"("_subs_",""
    for {
    . set sub=$order(@(prefix_$$Q(sub)_")")) quit:sub=""
    . new newSubs if subs'="" set newSubs=subs_","_$$Q(sub) else  set newSubs=$$Q(sub)
    . new subCount set subCount=$length(newSubs, ",")
    . if subCount<keyLength {
    . . do Traverse(gref, newSubs, keyLength, className, outputProps, .recCount, .fieldNodeMap, .fieldPieceMap, .fieldTypeMap)
    . } elseif subCount=keyLength {
    . . new fullRef set fullRef=gref_"("_newSubs_")"
    . . do ProcessNode(fullRef, className, outputProps, .recCount, .fieldNodeMap, .fieldPieceMap, .fieldTypeMap)
    . }
    }
    quit

; Parses node data and maps it to correct CSV fields
ProcessNode(ref, className, outputProps, recCount, fieldNodeMap, fieldPieceMap, fieldTypeMap)
    new line,prop,i,dataNode,dataPiece,type,dataStr,joinedVal,obj,sc,base,nodeRef,val
    set line=""
    new seq set seq=""
    for  set seq=$order(outputProps(seq)) quit:seq=""  {
    . new prop set prop=""
    . for  set prop=$order(outputProps(seq,prop)) quit:prop=""  {
    . set dataNode=fieldNodeMap(prop)
    . set dataPiece=fieldPieceMap(prop)
    . set type=fieldTypeMap(prop)
    . set base=$extract(ref,1,$length(ref)-1)
    . if dataNode'="" set nodeRef=$$NodeRef(base,dataNode) else  set nodeRef=ref
    . set val=$get(@nodeRef)
    . if dataPiece'="" {
    . . set dataStr=$piece(val,"^",dataPiece)
    . } else {
    . . set dataStr=val
    . }
    . if type'="" {
    . . if type'?1"%".E {
    . . . ; normal scalar
    . . } else {
    . . . if ##class(%Dictionary.ClassDefinition).%ExistsId(type) {
    . . . . set joinedVal=dataStr
    . . . . set obj=##class(type).%OpenId(dataStr,.sc)
    . . . . if $isobject(obj) {
    . . . . . if $property(obj,"Name")'="" set joinedVal=$property(obj,"Name")
    . . . . . if $property(obj,"ID")'="" set joinedVal=$property(obj,"ID")
    . . . . }
    . . . . set dataStr=joinedVal
    . . . }
    . . }
    . }
    . set dataStr=$replace(dataStr,"""","""""")
    . set dataStr=""""_dataStr_""""
    . if line'="" set line=line_","_dataStr  i '$l($g(line)) set line=dataStr
    }
    set recCount=recCount+1
    set ^EXPORT($J,recCount)=line
    do Log("Row "_recCount_" written.",2)
    quit

; Write CSV file from ^EXPORT($J)
WriteCSV(className,worker)
    new file,filePath,line,rec
    set filePath="C:\\InterSystems\\Cache\\Hackensack\\"_className_"_"_worker_".csv"
    set file=##class(%File).%New()
    set file.Name=filePath
    if file.Exists() do file.Delete()
    set sc=file.Open("WNS") if $SYSTEM.Status.IsError(sc) { write "ERROR opening file",! quit }
    set rec=""
    for  set rec=$order(^EXPORT($J,rec)) quit:rec=""  {
        set line=^EXPORT($J,rec)
        ;k ^EXPORT($J,rec)
        do file.WriteLine(line)
    }
    do file.Close()
    do Log("CSV file written to "_filePath,1)
    quit

; Quote wrapper for string subscripts
Q(x) if x'?1N.N {quit "\""_x_"\""} else {quit x}

; Safely retrieve a property value or return ""
GetProp(obj,name)
    new val
    set val=""
    try {
        set val=$property(obj,name)
    } catch {
        set val=""
    }
    quit val

; Build a node reference using base and storage node specification
NodeRef(base,node)
    new i,part,cnt,ref set ref=base
    if node'="" {
    . set cnt=$length(node,",")+1
    . for i=1:1:cnt {
    . . set part=$piece(node,",",i)
    . . set ref=ref_","_$$Q(part)
    . }
    }
    set ref=ref_")"
    quit ref

; Simple logging wrapper respecting gLogLevel
Log(msg,level)
    if $get(gLogLevel,1)<level quit
    write msg,!
    quit

; Job spawner stub — can be enhanced for controller logic
RunExportJobs(pGlobal, pClass, pKeyList, pLogLevel, pWorkers)
    new key,count,keyLen,setRoot,worker,total,gLogLevel
    set total=$select($get(pWorkers)>0:pWorkers,1:1)
    set gLogLevel=$get(pLogLevel,1)
    ; Determine how many subscripts make up the key
    set keyLen=1
    if pKeyList?1N.N set keyLen=pKeyList
    if pKeyList'?1N.N set keyLen=$length(pKeyList,",")
    do Log("Parent starting export using key length "_keyLen_" with "_total_" workers",1)
    for worker=1:1:total {
        job StartWorker(pGlobal,pClass,pKeyList,gLogLevel,worker,total)
        set count=count+1
        do Log("Launched worker "_worker,1)
    }
    do Log(count_" worker jobs launched.",1)
    quit

; Worker job processes a subset of keys
StartWorker(pGlobal,pClass,pKeyList,pLogLevel,worker,total)
    new idx,key,setRoot
    set idx=0,key=""
    for {
        set key=$order(@(pGlobal_"("_$$Q(key)_")")) quit:key=""  do
        . set idx=idx+1
        . continue:(idx-1)#total'=worker-1
        . set setRoot=pGlobal_"("_$$Q(key)_")"
        . do DrillTimeThree(setRoot,pClass,pKeyList,pLogLevel,worker,total)
    }
    quit
